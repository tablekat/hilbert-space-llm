I'll analyze the feasibility of using Hilbert space representations for token embeddings in neural networks, particularly in an LLM-transformer context. This will include:

- Mathematical formalization of representing tokens in function spaces (L² and other possible options).
- Transforming conventional matrix operations into operator-based formulations on Hilbert spaces.
- Evaluating the theoretical advantages and disadvantages of this approach compared to finite-dimensional embeddings.
- Exploring the potential for frequency-space inference and training via Fourier transforms.
- Assessing computational feasibility and efficiency.
- Reviewing existing work in related areas such as Fourier neural operators, kernel methods, and functional representations in deep learning.

I'll get back to you with a structured evaluation soon.

# Representing Tokens as Hilbert Space Elements in Transformers

## Mathematical Formalization of Token Representations in Function Spaces

**Tokens in L² and Other Function Spaces:** In a traditional transformer, each token is represented as a finite-dimensional vector (an embedding in $\mathbb{R}^d$). To generalize this, we can map tokens to functions in a Hilbert space (an infinite-dimensional complete inner-product space). For example, consider $H = L^2(D)$, the space of square-integrable functions on some domain $D$ (e.g. $D=[0,1]$ or a frequency interval). Each discrete token $t$ would be associated with a function $f_t(x) \in L^2(D)$. The inner product between two token-functions is defined as $\langle f_{t_i}, f_{t_j}\rangle = \int_D f_{t_i}(x)\,f_{t_j}(x)\,dx$ ([](https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture22.pdf#:~:text=v%2C%20w%20%E2%88%88%20V%20%3D%E2%87%92,R)). This setup is analogous to an **infinite-dimensional embedding**: we can think of $f_t(x)$ as an infinite coordinate vector for token $t$, where the inner product plays the role of cosine similarity/generalized dot-product. In fact, this is exactly the construction used in kernel methods, where one defines a feature map $\Phi(t)$ into a **reproducing kernel Hilbert space (RKHS)** such that $\langle \Phi(t_i), \Phi(t_j)\rangle_H = k(t_i,t_j)$ for some kernel $k$ ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=define%20a%20%E2%80%9Cfeature%20map%E2%80%9D%20%CE%A6,point%20gives%20rise%20to%20the)). Here $\Phi(t)$ would be our $f_t(x)$. In principle, any Hilbert space (e.g. an RKHS with a suitable kernel, or a Sobolev function space if smoothness is desired) could serve as the embedding space. The choice of function space can vary: for instance, one could use an **$l^2$ sequence space** (square-summable sequences, effectively an infinite-dimensional vector), an $L^2$ space of real functions, or a **wavelet/Fourier basis space**. Each token’s embedding could be expanded in some orthonormal basis $\{\phi_n(x)\}_{n=1}^\infty$, so $f_t(x) = \sum_{n} c_{t,n}\,\phi_n(x)$. A **finite approximation** would truncate this series to the first $N$ terms, recovering a length-$N$ vector of coefficients $(c_{t,1},\dots,c_{t,N})$ as in standard embeddings. The key difference is that $N$ can be made arbitrarily large (in the limit, infinite), and the basis functions $\phi_n$ can impart useful structure (e.g. frequency components).

**Linear Transformations as Operators on a Hilbert Space:** Neural network layers (affine transformations) that act on embeddings must be reinterpreted as **linear operators** on the function space. In a finite $d$-dimensional setting, a weight matrix $W \in \mathbb{R}^{d'\times d}$ acting on a vector $v \in \mathbb{R}^d$ produces $Wv$. In the Hilbert space $H$, a linear transformation becomes a bounded linear operator $A: H \to H$. If $H=L^2(D)$, any reasonable linear operator can be represented as an **integral kernel operator**: 
$$(A f)(s) \;=\; \int_D K(s, x)\, f(x)\,dx,$$ 
for some kernel function $K(s,x)$ which plays the role of an infinite weight matrix ([[PDF] the eigenvalue problem for a class of](https://maths.qmul.ac.uk/~ig/MAS214/int-ops-lect.pdf#:~:text=,%E2%89%A4%20x%2C%20y%20%E2%89%A4%20b)). This is directly analogous to matrix multiplication, with $K(s,x)$ generalizing the matrix entries and the integral generalizing the summation ([](https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture22.pdf#:~:text=Given%20this%20basic%20definition%20of,as%20vectors%20and%20operators%20as)). For example, if we choose $\phi_n(x)$ as an orthonormal basis for $H$, then $K(s,x)$ has the expansion $K(s,x) = \sum_{i,j} W_{ij}\,\phi_i(s)\,\phi_j(x)$, and $(Af)(s) = \sum_{i,j} W_{ij} \langle f,\phi_j\rangle\,\phi_i(s)$ recovers the usual matrix formula in the basis. Nonlinear activation functions would still act pointwise **on the coefficients** or on function values at a given point in some representation.

**Attention as an Operator in Hilbert Space:** The transformer's attention mechanism can also be reformulated in this functional view. In a standard transformer, **scaled dot-product attention** computes attention weights using dot-products of query and key vectors. If queries and keys are functions $q_i(x), k_j(x) \in H$, their similarity can be defined by the Hilbert space inner product $\langle q_i, k_j\rangle = \int q_i(x)k_j(x)\,dx$ ([](https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture22.pdf#:~:text=v%2C%20w%20%E2%88%88%20V%20%3D%E2%87%92,R)). The attention score (before softmax) between token $i$ and $j$ is then $a_{ij} = \langle q_i, k_j\rangle / \sqrt{\lambda}$ (with $\sqrt{\lambda}$ a scaling factor). These scores are passed through softmax to yield weights $\alpha_{ij}$. The output at token $i$ is a weighted combination of value functions: $z_i(x) = \sum_{j} \alpha_{ij}\, v_j(x)$. Here $v_j(x)$ is the value function for token $j$, and $z_i(x)$ is the resulting function embedding for token $i$ after the attention layer. This summation of functions is well-defined in $H$ because it’s just a linear combination of functions. We can interpret the attention mechanism as an **operator that maps the set of input functions $\{v_j\}$ to an output function $z_i$ for each $i$**, via those inner products. In the limit of a **continuum of tokens** (imagine $i$ being a continuous index on an interval rather than a discrete position), the attention operation would approach an *integral transform*. In fact, recent theoretical work has shown that one can derive a **continuum limit of transformers** where attention becomes an integral equation ([](https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf#:~:text=which%20is%20precisely%20the%20single,over%20multiple%20functions%20with%20form)). Formally, one can write an attention operator $\mathcal{A}$ acting on a continuous input function $v(j,x)$ (with $j$ now a continuous index playing the role of token position) as:
$$(\mathcal{A}v)(i,x) = \sigma\Big( v(i,x) + \int \mathrm{softmax}_j\{\langle q(i,\cdot),\,k(j,\cdot)\rangle\} \, v(j,x)\,dj \Big),$$ 
which in discrete form reduces to the usual $z_i(x) = \sigma\Big(v_i(x) + \sum_j \alpha_{ij} v_j(x)\Big)$ (ignoring normalization and multi-head details) ([](https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf#:~:text=which%20is%20precisely%20the%20single,over%20multiple%20functions%20with%20form)). This indicates that **dot-product attention is compatible with a Hilbert space setting** – it’s essentially using the Hilbert inner product as the similarity measure. Prior research has indeed characterized transformer attention as a form of kernel operation: it was shown that dot-product attention corresponds to a *kernel* (albeit an *indefinite*, asymmetrical one) with effectively **infinite-dimensional feature mappings** ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=We%20experiment%20with%20new%20kernels,understood%20model%20in%20modern%20machine)) ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=In%20this%20paper%2C%20we%20propose,not%20require%20the%20classic%20assumptions)). In summary, all mathematical operations in a transformer (linear projections, dot-products, softmax-weighted sums) can be reformulated in a functional setting. Tokens become elements of an inner-product function space, linear layers become linear functionals or integral transforms, and attention can be seen as a kernel-based operator gluing these functions together.

## Theoretical Benefits and Drawbacks of Infinite-Dimensional Token Representations

**Expressivity and Representational Power:** Representing tokens in an infinite-dimensional Hilbert space can, in theory, **increase the model’s expressivity and capacity**. An infinite (or very high) dimensional feature space can disentangle data in ways finite dimensions might not. Any finite embedding can be seen as a *projection* of the token’s “true” identity; an infinite-dimensional function can potentially capture far more nuanced features or relationships. This is analogous to how kernel methods make data linearly separable by mapping it to a high-dimensional (often infinite-dimensional) feature space. In fact, the success of transformers has been partly attributed to behavior akin to an infinite feature space: one study noted that the transformer's attention kernel has *infinite feature dimension*, which appears to be **“partially responsible for its performance”** ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=We%20experiment%20with%20new%20kernels,understood%20model%20in%20modern%20machine)). With a Hilbert space representation, tokens could encode rich structures – for example, subtle semantic features might be represented along different functional directions, or a token could occupy a volume in the space (if we use distribution embeddings). This added richness might allow the model to capture relationships that are challenging for finite vectors. For instance, *density-based embeddings* like **Gaussian word embeddings** (which represent each word as a Gaussian distribution rather than a point vector) have demonstrated the ability to capture uncertainty and asymmetry in meaning relationships ([[1412.6623] Word Representations via Gaussian Embedding](https://arxiv.org/abs/1412.6623#:~:text=a%20point%20vector%20in%20low,explore%20novel%20properties%20of%20the)). Such distributions live in an infinite-dimensional function space (the space of probability density functions), and they enable more expressive decision boundaries than point vectors ([[1412.6623] Word Representations via Gaussian Embedding](https://arxiv.org/abs/1412.6623#:~:text=a%20point%20vector%20in%20low,explore%20novel%20properties%20of%20the)). Generally, an infinite basis can approximate any finite feature combination and more – so there is a **theoretical universality** in representation. One might expect this to improve the model’s ability to fit complex functions (potentially aiding **convergence** by giving the optimizer more degrees of freedom to find a good representation), and possibly improve generalization if relevant functional features of the data are captured (especially if combined with proper regularization).

**Comparisons to Finite Embeddings:** In a standard finite embedding, there is always a trade-off between dimensionality and performance; too few dimensions and the model is under-powered, too many and it becomes inefficient or overfits. Using a Hilbert space essentially says “let’s take dimensionality to the limit.” In theory, any finite embedding approach could be seen as a special case (by choosing a subspace of the Hilbert space spanned by a finite basis). Thus, the Hilbert space representation is *at least* as expressive as finite vectors, and strictly more expressive if one truly leverages the additional dimensions. Another way to view this is through the **kernel trick**: any positive-definite similarity measure between tokens can be realized as an inner product in some (possibly infinite) feature space ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=define%20a%20%E2%80%9Cfeature%20map%E2%80%9D%20%CE%A6,point%20gives%20rise%20to%20the)). Standard transformers implicitly learn their own embedding space; a Hilbert space approach could allow us to *prescribe* a feature space with desirable properties (e.g. smooth functions, or ones that emphasize certain frequencies). However, with great expressivity comes the risk of **overfitting**. Infinite-dimensional spaces can represent functions that vary arbitrarily fast or pick up noise – without constraints, a model could memorize training data by shuffling around in an enormous feature space. Finite networks somewhat mitigate this by implicit regularization (e.g. weight decay, limited capacity), whereas an infinite representation would require explicit regularization (e.g. norm constraints in the Hilbert space) to control complexity ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=core%20of%20the%20Transformer%E2%80%99s%20operation,functions%20learned%20are%20elements%20of)) ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=Hilbert%20spaces%3A%20as%20prior%20work,their%20additional%20nonlinearity%20and%2For%20nonconvexity)). The **convergence behavior** of training might be affected in complex ways: more degrees of freedom could mean a larger hypothesis space to search, potentially making optimization harder *unless* structure is imposed. On the other hand, if the chosen Hilbert space aligns well with the task (for example, using an RKHS with a kernel tailored to language data), one might converge faster or with less data because the features are more natural for the problem. This is similar to how using a good kernel in an SVM can simplify the learning problem. In summary, compared to finite embeddings, Hilbert space embeddings offer *increased theoretical expressivity* and the possibility to naturally encode prior knowledge (through the choice of function space), at the expense of a much **higher risk of complexity and overfitting** if not handled carefully.

**Potential Efficiency Gains (and Pitfalls):** It might seem counter-intuitive, but working in a function space could offer *some* efficiency advantages in special cases. If operations in the network correspond to known transforms, we can exploit analytical or computational shortcuts. A prime example is doing **inference or training in the frequency domain**. If token representations are functions (say of time or space), we can apply a **Fourier transform** to convert convolution-like operations into multiplicative ones, or to diagonalize certain linear operators. In fact, if a linear operator $A$ is convolutional (translation-invariant in the domain $D$), then in the Fourier basis it becomes a simple pointwise multiplication (by the transfer function). This insight underpins the **Fourier Neural Operator (FNO)** approach, which learns global convolution operators in the frequency domain for efficiency ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=challenge%2C%20we%20propose%20Adaptive%20Fourier,FNO%20which%20results%20in%20memory)). In a transformer context, one piece of prior work (the **FNet** model) demonstrated that you can *replace the attention mechanism entirely with a Fourier transform* (a fixed linear mixing of the input) and still retain most of the accuracy, while significantly improving speed ([](https://aclanthology.org/2022.naacl-main.319.pdf#:~:text=input%20tokens,the%20%E2%80%9Cefficient%20Transformers%E2%80%9D%20on%20the)) ([](https://aclanthology.org/2022.naacl-main.319.pdf#:~:text=find%20that%20the%20Fourier%20Transform%2C,2014%3B%20Highlander%20and)). FNet mixes token representations by applying a Fourier transform across the sequence dimension, achieving $O(n \log n)$ complexity (due to FFT) instead of $O(n^2)$ attention, and managed to get **92-97% of BERT’s accuracy while training ~80% faster** on typical sequence lengths ([](https://aclanthology.org/2022.naacl-main.319.pdf#:~:text=input%20tokens,the%20%E2%80%9Cefficient%20Transformers%E2%80%9D%20on%20the)). This suggests that **frequency-space operations can capture much of the “token mixing” that attention provides** ([](https://aclanthology.org/2022.naacl-main.319.pdf#:~:text=scheme%2C%20we%20further%20investigate%20the,approximate%20or%20speed%20up%20computations)). If tokens were represented as functions (e.g. as signals over some latent frequency domain), one could imagine performing parts of the transformer in that domain – for instance, computing attention or other interactions via convolution theorem, or focusing on important frequency components. Another potential benefit is that continuous function representations might naturally handle **variable sequence lengths or resolutions**. If we treat the sequence as a piecewise or continuous function, extending context length or interpolating between positions could be more seamless than with fixed-size vectors. Some recent transformer variants use *continuous positional representations* or memory compressions to allow virtually unlimited context ([Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/html/2404.07143v1#:~:text=This%20work%20introduces%20an%20efficient,parameters%20and%20enables%20fast%20streaming)) ([Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/html/2404.07143v1#:~:text=The%20attention%20mechanism%20in%20Transformers,context%20models%20becomes%20costly%20financially)); representing everything in a continuous Hilbert space could conceptually unify such approaches.

**Key Drawbacks and Challenges:** Despite the theoretical appeal, several drawbacks are apparent. The **infinite dimensionality** is a double-edged sword: while it offers expressive power, it is *not computationally tractable* to represent or manipulate truly infinite-dimensional objects on actual hardware. In practice, one would have to **truncate or approximate** the function representations. This effectively brings the model back to a high but finite dimension, potentially negating some benefits if the truncation is severe. Moreover, high-dimensional representations suffer from the **curse of dimensionality**: more dimensions mean more parameters and more data needed to reliably optimize them. There is also the issue of **efficiency** in basic operations. Unless the structure of the Hilbert space and the operators is exploited, working with functions can be far more expensive than with vectors. For example, multiplying two vectors of size $d$ is $O(d)$, but composing two functions might involve integrals that are expensive to compute unless they simplify (e.g. via FFT or known kernels). If our function space is $L^2([0,1])$ and we have no further structure, computing an inner product $\int_0^1 f(x)g(x)dx$ might require numeric integration or a large number of basis coefficients – *no better (and likely worse) than computing a high-dimensional dot product*. So, a naive Hilbert-space transformer could be **orders of magnitude slower** than a finite one. Even attention, reformulated as above, would require computing and softmax-normalizing an infinite array of inner products if done naively. One study explicitly notes that in the continuum limit, the transformer’s attention would require evaluating a “nested integral for each $x$,” leading to high computational complexity ([](https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf#:~:text=Even%20though%20transformers%20are%20special,However%2C%20many%20of%20the)). Thus, any efficiency benefits of the frequency domain or other tricks have to outweigh the overhead of handling functional data. Another theoretical drawback is **lack of well-understood generalization**: deep learning in infinite-dimensional spaces is not as well-charted by theory. While kernel machines in RKHS have clear generalization bounds (often requiring regularization), a deep transformer with millions of parameters operating in a Hilbert space is a more mysterious beast. It may require new learning theory developments (perhaps in *reproducing kernel Banach spaces*, as some work has begun exploring ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=core%20of%20the%20Transformer%E2%80%99s%20operation,functions%20learned%20are%20elements%20of))). Finally, one must consider **numerical stability**. High-dimensional expansions (e.g., Fourier series with many coefficients) can be prone to numerical error, and ensuring that operations like softmax (which could be integrating exponentials of inner products) remain stable might need careful analysis. In summary, the approach promises greater expressive power and some computational shortcuts (via structure like Fourier transforms), but suffers from potential inefficiency and overfitting **unless carefully structured**. The gains in expressivity might or might not translate to practical improvement, especially if a finite model can already approximate the needed functions. As we have seen, even very large but finite models (with say, 10k-dimensional embeddings) already perform remarkably well in NLP, so the *incremental benefit of “truly infinite” representations is still speculative*.

## Computational Feasibility and Practical Considerations

**Implementation Challenges:** Implementing a transformer with Hilbert space embeddings is non-trivial. The fundamental problem is that we cannot explicitly store an infinite-dimensional object. Therefore, any implementation must choose a *representation scheme* for the token functions. Possible approaches include: (1) **Truncated basis expansion:** choose a fixed orthonormal basis $\{\phi_1,\phi_2,\dots,\phi_N\}$ in the function space and represent each token by its first $N$ coefficients $(c_1,\ldots,c_N)$. This reduces the problem back to a standard transformer with embedding dimension $N$. While this is straightforward, it limits the benefits – effectively $N$ becomes the new embedding size, and one might simply treat it as a large embedding dimension. One would need to make $N$ very large to approach “infinite” capacity, which increases memory and compute costs linearly in $N$. (2) **Analytic function representations:** assign each token a functional form with finite parameters (for instance, a Gaussian $f_t(x)=\exp(-(x-\mu_t)^2/\sigma_t^2)$ defined by mean $\mu_t$ and variance $\sigma_t$). This keeps representation finite (just a few parameters per token) but allows one to compute overlaps/integrals in closed-form. However, using simple parametric functions might constrain expressivity too much or be hard to tune. (3) **Implicit kernel representations:** avoid explicit functions and instead define operations via kernel evaluations. For example, if we choose an RKHS with kernel $k(x,y)$, we can compute inner products $\langle f_{t_i}, f_{t_j}\rangle = k(\omega_i,\omega_j)$ if each token’s feature is itself indexed by some parameter $\omega$. This is akin to representing each token by a point in some latent space and a fixed kernel. The **Performer** architecture is a relevant example – it replaces the softmax attention with an *explicit kernel feature map*, using random Fourier features to approximate the Gaussian kernel underlying softmax attention ([[2105.03824] FNet: Mixing Tokens with Fourier Transforms - ar5iv](https://ar5iv.labs.arxiv.org/html/2105.03824#:~:text=ar5iv%20ar5iv,7%2C%202021)). By doing so, it implicitly lifts the representation to an infinite-dimensional space (the feature map of an RBF kernel) but without ever storing an infinite vector; it computes attention weights via kernel computations. Such kernel-based tricks could be extended: one could design a kernel for tokens (e.g. based on token identity or attributes) and then build the transformer to operate on kernel values. The challenge is that nonlinear transformations (like the feed-forward MLP in a transformer block) don’t have an easy kernel trick analogue – we would have to linearize or approximate them.

**Computational Cost and Scalability:** If implemented straightforwardly (e.g. with a large truncation dimension $N$), the cost of a Hilbert space transformer will be substantially higher than a standard one. Every vector-matrix multiplication becomes an $N$-dimensional integral or $N$-term sum, and attention scales as $O(n^2 \cdot N)$ if $n$ is sequence length. Without special structure, both memory and time blow up. That said, researchers have found ways to make *certain* infinite-dimensional operations efficient. The **Fourier Neural Operator (FNO)** and its vision counterpart **Adaptive Fourier Neural Operator (AFNO)** exploit the structure of convolution in function spaces to achieve *quasi-linear* complexity in sequence length ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=challenge%2C%20we%20propose%20Adaptive%20Fourier,FNO%20which%20results%20in%20memory)) ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=propose%20principled%20architectural%20modifications%20to,attention%20mechanisms)). AFNO, for instance, frames token mixing as a **continuous global convolution** in the Fourier domain, which **removes dependence on input resolution** (sequence length) ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=challenge%2C%20we%20propose%20Adaptive%20Fourier,FNO%20which%20results%20in%20memory)). By doing so, AFNO can handle extremely large sequences (65k tokens/pixels) with only linear memory growth, outperforming standard self-attention in both speed and accuracy for very high-resolution data ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=propose%20principled%20architectural%20modifications%20to,attention%20mechanisms)). This is an encouraging sign for scalability: it suggests that if the operations can be structured (here as convolution in a latent continuous space), one can mitigate the normally crippling $O(n^2)$ complexity. Another strategy is to impose **low-rank or sparse structures** on the operators. For example, one might assume that the linear operators (weight matrices) have a low-rank kernel in the function basis, or that only the first $M$ basis functions are needed for interactions, etc. This is analogous to how we sometimes compress large embedding matrices via SVD or limit attention to local windows in long sequences.

**Memory Considerations:** Storing a function (even approximately) means storing a lot of coefficients or parameters. If each token’s representation requires $N$ numbers and $N$ is large, memory usage will balloon. Training a transformer already consumes a lot of memory for activations and gradients; an infinite-dimensional version would exacerbate this unless $N$ is kept moderate or the representation is implicit. Some implicit representations (like kernel methods) shift memory burden to storing pairwise similarities or basis functions. Another consideration is **GPU/TPU support** – current deep learning hardware and libraries are optimized for dense matrix operations on relatively modest dimensions (usually up to a few thousands). Extremely high-dimensional operations might not be efficient due to poor cache use or simply exceeding hardware capabilities. One would likely need to develop new kernels (in the CUDA sense) for operations like integrals or Fourier transforms on the fly. The Fourier transform itself is well-supported (CUFFT etc.), so approaches leveraging FFT (like FNO/AFNO or FNet) can run efficiently on hardware ([](https://aclanthology.org/2022.naacl-main.319.pdf#:~:text=find%20that%20the%20Fourier%20Transform%2C,2014%3B%20Highlander%20and)).

**Training Feasibility:** Training a model in an infinite-dimensional space raises questions of optimization. Stochastic gradient descent still works in Hilbert spaces in theory (it’s the basis of functional gradient methods), but convergence might be slow if the parameter space is huge. In practice, one would be training the finite parameters that define the functions (basis coefficients or other param reps), so it reduces to standard training with more parameters. One must be cautious about regularization – it could be important to penalize the **Hilbert space norm** of the token functions (to avoid pathological solutions). This might entail adding terms that penalize $\|f_t\|_{H}^2 = \langle f_t, f_t\rangle$ for embeddings, or using weight decay on the coefficients. Without such regularization, the model might exploit the extra degrees of freedom in undesirable ways. From a *viability* standpoint, it’s worth noting that even very large finite models (billions of parameters) are trainable given enough compute. An infinite-dimensional model can often be viewed as the limit of increasing some layer width or embedding size. For example, **neural network Gaussian Processes** arise as width $\to \infty$, and though we don’t explicitly train infinite networks (we train finite large ones), the behavior tends toward that infinite limit. By analogy, one could start with a reasonably large $N$ (embedding dimension or number of basis functions) and see improvements as $N$ grows, stopping when returns diminish. If the task truly benefits from more complex token representations, some intermediate $N$ might yield gains. However, the **diminishing returns** and the massive compute required might make fully infinite representations impractical except for toy problems. 

In summary, a direct implementation of Hilbert-space token embeddings in a transformer is computationally **very demanding**, but there are pathways to make it tractable: exploiting structure (Fourier/domain-specific transforms), using implicit representations (kernels or parametric functions), and leveraging hardware-optimized operations. Current evidence from related research (see below) shows that certain continuous or infinite-dimensional techniques can be scaled to reasonably large problem sizes, but integrating all aspects (token, position, attention) in a single unified Hilbert-space model would be a significant engineering challenge.

## Related Work and Existing Research

Although the idea of *explicitly* representing tokens in an infinite-dimensional space has not been a mainstream approach in NLP, several **related areas of research** provide insight or partial solutions:

- **Kernel Methods and Implicit Infinite Features:** In classical machine learning, kernel methods (SVMs, Gaussian processes, etc.) effectively work in high- or infinite-dimensional feature spaces without ever computing explicit coordinates. Each data point (analogous to a token) is represented via a kernel function $k(x,\cdot)$ in an RKHS ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=define%20a%20%E2%80%9Cfeature%20map%E2%80%9D%20%CE%A6,point%20gives%20rise%20to%20the)). There have been efforts to combine kernel methods with deep learning; for example, the transformer’s attention can be viewed as a kernel machine ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=In%20this%20paper%2C%20we%20propose,not%20require%20the%20classic%20assumptions)). The Performer architecture approximates the softmax attention kernel with random Fourier features, implicitly using an *RBF kernel feature space* for tokens ([[2105.03824] FNet: Mixing Tokens with Fourier Transforms - ar5iv](https://ar5iv.labs.arxiv.org/html/2105.03824#:~:text=ar5iv%20ar5iv,7%2C%202021)). This allowed attention to be computed in *linear time* by avoiding explicit $n \times n$ attention matrices. Similarly, the **FourierFormer** (Nguyen et al., 2022) reinterprets attention as a form of **nonparametric kernel regression**, replacing the dot-product with a *generalized Fourier integral kernel* to better capture query-key dependencies ([FourierFormer: Transformer Meets Generalized Fourier Integral Theorem | OpenReview](https://openreview.net/forum?id=PRd7VG_ki_#:~:text=Abstract%3A%20Multi,are%20replaced%20by%20the%20novel)) ([FourierFormer: Transformer Meets Generalized Fourier Integral Theorem | OpenReview](https://openreview.net/forum?id=PRd7VG_ki_#:~:text=generalized%20Fourier%20integral%20kernels,transformers%20in%20a%20variety%20of)). FourierFormer can be seen as designing a specific (infinite-dimensional) feature mapping for queries/keys so that attention weights are computed via integrating those features (leveraging Fourier analysis); it achieved improved accuracy and reduced redundant heads compared to standard transformers ([FourierFormer: Transformer Meets Generalized Fourier Integral Theorem | OpenReview](https://openreview.net/forum?id=PRd7VG_ki_#:~:text=generalized%20Fourier%20integral%20kernels,transformers%20in%20a%20variety%20of)). These works show that *injecting infinite-dimensional kernel features into transformers is feasible and can even be beneficial*. They typically maintain efficiency by either using analytical kernel formulas or low-dimensional random projections to approximate the infinite feature space.

- **Fourier Neural Operators and Continuous Representations:** The **Fourier Neural Operator (FNO)** by Li et al. (2021) introduced a paradigm for learning mappings **between function spaces**, primarily to solve PDEs. FNOs treat the input as a function sampled on a grid, apply a Fourier transform, multiply by learned filters in frequency space, and transform back – repeating this to propagate information globally ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=challenge%2C%20we%20propose%20Adaptive%20Fourier,FNO%20which%20results%20in%20memory)). Essentially, this learns an operator that **acts on functions** rather than finite vectors, and it has been proven effective for tasks like modeling fluid dynamics. In the context of transformers, researchers have drawn parallels: one paper showed that a standard transformer can be viewed as a specific kind of neural operator in the limit of many tokens, and derived a *continuum version of the transformer* equations ([](https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf#:~:text=which%20is%20precisely%20the%20single,over%20multiple%20functions%20with%20form)). Vision Transformers for high-resolution images have adopted FNO-inspired approaches; the **AFNO** method (Adaptive Fourier Neural Operator) replaced self-attention with a Fourier domain operator that mixes image patches as continuous functions ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=Abstract%3A%20Vision%20transformers%20have%20delivered,to%20design%20FNO%2C%20which%20solves)) ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=learning%20challenging%20PDEs,For%20Cityscapes)). By doing so, AFNO achieves near-linear scaling and was able to handle extremely large image sizes (equivalent to very long token sequences) with less compute ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=propose%20principled%20architectural%20modifications%20to,attention%20mechanisms)). These works demonstrate *successful training of networks that operate on functional representations*. While FNO/AFNO are specialized to certain tasks, they confirm that one can treat data as functions (in a Hilbert space of Fourier series) and learn deep models on them. A related line of work is **DeepONet (Deep Operator Network)**, which learns arbitrary nonlinear operators between function spaces by using two networks – one to encode the input function (via evaluations at a set of sensor points) and another to encode the query location – then merging them ([DeepONet: A deep neural network-based model to approximate ...](https://techxplore.com/news/2021-04-deeponet-deep-neural-network-based-approximate.html#:~:text=,both%20linear%20and%20nonlinear%20operators)). DeepONet and its successors have shown it’s possible to train deep models where *inputs and outputs are functions* (infinite-dimensional objects) by appropriate finite parametrizations. This is conceptually similar to what a “Hilbert space transformer” would do (the transformer being a learned operator mapping an input function – or sequence of token functions – to an output function representation).

- **Functional Data and Distributional Representations:** Outside of transformers, there is a rich field of **functional data analysis** and some recent work on bringing deep learning to functional data. For example, **functional autoencoders** and RNNs have been developed to take in time-series or curves (modeled as functions) and learn representations without first converting them to finite vectors ([[PDF] Deep Learning for Functional Data Analysis with Adaptive Basis ...](http://proceedings.mlr.press/v139/yao21c/yao21c.pdf#:~:text=,vector%20and%20then%20carry)) ([Functional data analysis using deep neural networks - Wang - 2024](https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.70001#:~:text=Functional%20data%20analysis%20using%20deep,networks%20with%20functional%20data%20analysis)). These models essentially have to tackle the same issues of representing and computing with functions (often by expanding in basis or using discretization). In NLP specifically, *distributional representations* of meaning go beyond point embeddings: we mentioned Gaussian embeddings for words ([[1412.6623] Word Representations via Gaussian Embedding](https://arxiv.org/abs/1412.6623#:~:text=a%20point%20vector%20in%20low,explore%20novel%20properties%20of%20the)); others have proposed **multi-modal embeddings** (like a word is a region or a subset in embedding space, effectively a function indicating a set). More generally, **Hilbert space embeddings of probability distributions** (the kernel mean embedding) is a technique where an entire probability distribution is represented as *one element in an RKHS* ([Kernel embedding of distributions - Wikipedia](https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions#:~:text=Kernel%20embedding%20of%20distributions%20,a%20class%20of%20nonparametric%20methods)). This has been used in two-sample testing, Bayesian inference, and model criticism. If one views each token as conveying a distribution over meanings or contexts, one could use such *kernel mean embeddings* to represent tokens in an infinite-dimensional feature space (the RKHS of that kernel). There is existing research on comparing distributions via Hilbert space embeddings and even training models on those embeddings ([[PDF] 22 : Hilbert Space Embeddings of Distributions](https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture22.pdf#:~:text=,is%20a%20infinite%20dimensional)) ([Kernel mean embeddings in Reproducing Kernel Hilbert Spaces ...](https://math.stackexchange.com/questions/4859187/kernel-mean-embeddings-in-reproducing-kernel-hilbert-spaces-linear-kernel#:~:text=Kernel%20mean%20embeddings%20in%20Reproducing,measure%20P%20defined%20on%20X)), though not specifically in transformers. It’s a parallel that suggests one could represent more information per token (like ambiguity or variance) using a functional representation.

- **Continuous Input Representations in Transformers:** There are some works that make transformers handle continuous data or infinite sequences. *Continuous-position transformers* replace discrete positional encodings with continuous functions, sometimes using Fourier features to allow attending to any position (this is related to infinite context length). The **Infinite Memory Transformer** and related efforts incorporate *continuous-time attention* to allow unbounded sequences ([Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/html/2404.07143v1#:~:text=This%20work%20introduces%20an%20efficient,Our)). While these focus on sequence length, not embedding dimensionality, they often involve representing the sequence as a continuous function of time, which is conceptually aligned with the idea of a functional representation. They show that the transformer architecture can be extended to settings where something is unbounded (here, time) by using clever compression or functional approximations.

In summary, **existing research provides building blocks toward Hilbert space token representations**: kernel methods provide the theory for infinite feature mappings, neural operator research shows how to train networks on functions (using Fourier transforms or pointwise evaluations), and various transformer variants have already incorporated continuous or kernel-based components to improve performance. To date, a full transformer that *natively works in an infinite-dimensional embedding space* hasn’t been demonstrated, but the pieces have been explored in isolation. The findings from these works are encouraging – e.g., *Fourier-based token mixing can replace attention with minimal accuracy loss and big speed gains* ([](https://aclanthology.org/2022.naacl-main.319.pdf#:~:text=input%20tokens,the%20%E2%80%9Cefficient%20Transformers%E2%80%9D%20on%20the)), *kernelized attention can improve flexibility* ([FourierFormer: Transformer Meets Generalized Fourier Integral Theorem | OpenReview](https://openreview.net/forum?id=PRd7VG_ki_#:~:text=generalized%20Fourier%20integral%20kernels,transformers%20in%20a%20variety%20of)), and *neural networks can approximate operators on function spaces with good accuracy* ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=challenge%2C%20we%20propose%20Adaptive%20Fourier,FNO%20which%20results%20in%20memory)) ([DeepONet: A deep neural network-based model to approximate ...](https://techxplore.com/news/2021-04-deeponet-deep-neural-network-based-approximate.html#:~:text=,both%20linear%20and%20nonlinear%20operators)). They also highlight limitations: each method imposes some structure to stay tractable (random feature approximations, band-limited Fourier representations, etc.), indicating that a successful Hilbert-space transformer would likely need similar assumptions or constraints.

## Conclusion and Future Directions

Representing tokens as elements of a Hilbert space is a **conceptually feasible** extension of the transformer model, offering a powerful unification of discrete sequence processing and functional representation. We can mathematically define how to do it (tokens mapped to $L^2$ functions or other Hilbert spaces, with linear layers as integral operators and attention as inner-product-based kernel operations). The potential upsides include vastly richer representations and new computational tricks (like working in the frequency domain) that could handle longer contexts or novel types of data. However, these benefits come with significant theoretical and practical challenges. Infinite-dimensional representations risk overfitting and are computationally expensive; without special structure, they are impractical. 

**In practice, the feasibility of this approach will depend on adopting approximate or hybrid strategies**. One promising direction is to combine **learned finite representations with functional transforms**: for example, a token could still be stored as a finite vector, but the transformer layers might internally treat it as coefficients of a functional basis (performing Fourier transforms or kernel evaluations to mix information). This hybrid approach is already evident in things like Performer (finite random features emulating an infinite kernel) and FNet/AFNO (finite FFT operations achieving effects of global convolution). Another direction is exploring **adaptive basis functions**: rather than a fixed truncation, the model could learn which basis functions are important for each token or each layer, effectively focusing the infinite basis on a manageable subset. This could be done with something like an attention-over-basis mechanism or using sparsity/attention to activate only parts of the function domain for a given interaction.

**Research opportunities** lie in developing *new architectures or training methods* that explicitly incorporate functional representations. For instance, one could imagine a **“functional embedding layer”** that learns a mapping from a token index to a continuous function (perhaps parameterized by a neural network itself). Each token might be represented by a small neural network (or a set of coefficients) that produces a function value given a query (somewhat like how DeepONet uses a network to represent a function). This would effectively turn the embedding lookup into a mini-function generator. If such functions are smooth or band-limited, one could use multi-resolution techniques (like wavelets) to make computation adaptive – coarse approximations for rough interactions and fine detail when needed. Another future direction is exploring **regularization techniques** specific to function spaces: for example, encouraging smoothness of token functions or limiting their bandwidth could control complexity and improve generalization, analogous to how kernel methods control RKHS norm. From a theoretical perspective, it would be valuable to understand the learning guarantees in these infinite-dimensional models – e.g., does training a transformer in a Hilbert space converge to a “kernel regression” solution in some limit, or can we prove a representer theorem that the optimal functions lie in the span of some finite set (which would justify truncation)? Early steps in this direction are being taken by linking transformers to kernel/Banach space learning theory ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=core%20of%20the%20Transformer%E2%80%99s%20operation,functions%20learned%20are%20elements%20of)).

In conclusion, representing tokens as Hilbert space elements is an **intriguing idea that bridges deep learning with functional analysis and kernel methods**. While a full-fledged infinite-dimensional transformer is not yet practical, research in related domains suggests it’s not an outlandish idea: attention already behaves like an infinite feature kernel ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=We%20experiment%20with%20new%20kernels,understood%20model%20in%20modern%20machine)), and continuous operator-learning models are trainable. The key will be to **capture the benefits of infinite-dimensional representations while sidestepping their computational burdens**. Techniques like Fourier transforms, random feature mappings, and learned basis truncation are likely to play a crucial role. As models continue to grow and researchers seek new inductive biases, the Hilbert space view offers a rich framework to think about what might lie beyond fixed-size vector embeddings. Future exploration in this direction could lead to models that naturally handle more complex data (like multimodal continuous signals or distributions) and that gracefully scale along dimensions that are currently problematic (such as context length or embedding size). In essence, it opens up a **new design space for neural architectures** – one where networks learn to manipulate not just vectors, but entire functions as first-class entities, potentially combining the strengths of deep learning and functional/kernel methods in one framework. 

**Sources:**

- Wright & Gonzalez (2021) – *Transformers are Deep Infinite-Dimensional Kernel Machines* ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=We%20experiment%20with%20new%20kernels,understood%20model%20in%20modern%20machine)) ([Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines](https://arxiv.org/pdf/2106.01506#:~:text=In%20this%20paper%2C%20we%20propose,not%20require%20the%20classic%20assumptions))  
- Kovachki et al. (2021) – *Neural Operator: continuum limit of transformers and FNO for PDEs* ([](https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf#:~:text=which%20is%20precisely%20the%20single,over%20multiple%20functions%20with%20form)) ([](https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf#:~:text=Even%20though%20transformers%20are%20special,However%2C%20many%20of%20the))  
- Choromanski et al. (2021) – *Rethinking Attention with Performers (linear kernel attention)* ([[2105.03824] FNet: Mixing Tokens with Fourier Transforms - ar5iv](https://ar5iv.labs.arxiv.org/html/2105.03824#:~:text=ar5iv%20ar5iv,7%2C%202021))  
- Lee-Thorp et al. (2022) – *FNet: Mixing Tokens with Fourier Transforms (efficient token mixing via FFT)* ([](https://aclanthology.org/2022.naacl-main.319.pdf#:~:text=input%20tokens,the%20%E2%80%9Cefficient%20Transformers%E2%80%9D%20on%20the)) ([](https://aclanthology.org/2022.naacl-main.319.pdf#:~:text=scheme%2C%20we%20further%20investigate%20the,approximate%20or%20speed%20up%20computations))  
- Guibas et al. (2022) – *AFNO: Adaptive Fourier Neural Operator for vision transformers* ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=challenge%2C%20we%20propose%20Adaptive%20Fourier,FNO%20which%20results%20in%20memory)) ([Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators | OpenReview](https://openreview.net/forum?id=EXHG-A3jlM#:~:text=propose%20principled%20architectural%20modifications%20to,attention%20mechanisms))  
- Nguyen et al. (2022) – *FourierFormer: attention via Fourier integral kernels* ([FourierFormer: Transformer Meets Generalized Fourier Integral Theorem | OpenReview](https://openreview.net/forum?id=PRd7VG_ki_#:~:text=Abstract%3A%20Multi,are%20replaced%20by%20the%20novel)) ([FourierFormer: Transformer Meets Generalized Fourier Integral Theorem | OpenReview](https://openreview.net/forum?id=PRd7VG_ki_#:~:text=generalized%20Fourier%20integral%20kernels,transformers%20in%20a%20variety%20of))  
- Vilnis & McCallum (2015) – *Word Representations via Gaussian Embedding (tokens as density functions)* ([[1412.6623] Word Representations via Gaussian Embedding](https://arxiv.org/abs/1412.6623#:~:text=a%20point%20vector%20in%20low,explore%20novel%20properties%20of%20the))  
- Muandet et al. (2017) – *Kernel Mean Embeddings (RKHS representations of distributions)* ([Kernel embedding of distributions - Wikipedia](https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions#:~:text=Kernel%20embedding%20of%20distributions%20,a%20class%20of%20nonparametric%20methods)) (lecture notes)  
- Park et al. (2024) – *Foundation Policies with Hilbert Representations (demonstrating Hilbert space embeddings in RL)* ([Foundation Policies with Hilbert Representations - arXiv](https://arxiv.org/html/2402.15567v1#:~:text=Foundation%20Policies%20with%20Hilbert%20Representations,Section%206)) (contextual mention)  
- Additional functional analysis references for definitions and analogies ([](https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture22.pdf#:~:text=v%2C%20w%20%E2%88%88%20V%20%3D%E2%87%92,R)) ([](https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture22.pdf#:~:text=Given%20this%20basic%20definition%20of,as%20vectors%20and%20operators%20as)) ([[PDF] the eigenvalue problem for a class of](https://maths.qmul.ac.uk/~ig/MAS214/int-ops-lect.pdf#:~:text=,%E2%89%A4%20x%2C%20y%20%E2%89%A4%20b)).
